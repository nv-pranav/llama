{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-05T01:20:45.776233Z",
     "start_time": "2024-06-05T01:20:45.769971Z"
    }
   },
   "source": "print(\"hi\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T01:20:50.110507Z",
     "start_time": "2024-06-05T01:20:49.058667Z"
    }
   },
   "cell_type": "code",
   "source": "import torch",
   "id": "e67726b6af35955",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T01:22:09.861707Z",
     "start_time": "2024-06-05T01:21:05.253167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from dummyModel import Transformer, ModelArgs\n",
    "torch.set_default_dtype(torch.float16) \n",
    "default_dtype = torch.get_default_dtype()\n",
    "print(default_dtype)  # This will print the default dtype\n",
    "\n",
    "def main():\n",
    "    # Initialize model arguments\n",
    "    model_args = ModelArgs(\n",
    "        dim=4096,\n",
    "        n_layers=32,\n",
    "        n_heads=32,\n",
    "        vocab_size=30522,  # Set the appropriate vocab size for your model\n",
    "        multiple_of=256,\n",
    "        ffn_dim_multiplier=None,\n",
    "        norm_eps=1e-5,\n",
    "        max_batch_size=32,\n",
    "        max_seq_len=2048\n",
    "    )\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Transformer(model_args).cuda()  # Move model to GPU if available\n",
    "    \n",
    "    for name, layer in model.named_modules():\n",
    "        print(name, layer)\n",
    "\n",
    "    # Create dummy input (batch_size=1, sequence_length=128)\n",
    "    dummy_input = torch.randint(0, model_args.vocab_size, (1, 128)).cuda()\n",
    "\n",
    "    # Run the forward pass and print layer details\n",
    "    output = model(dummy_input)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "main()\n"
   ],
   "id": "473e93985cd43904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      "torch.float16\n",
      " Transformer(\n",
      "  (tok_embeddings): Embedding(30522, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "        (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "        (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=4096, out_features=30522, bias=False)\n",
      ")\n",
      "tok_embeddings Embedding(30522, 4096)\n",
      "layers ModuleList(\n",
      "  (0-31): 32 x TransformerBlock(\n",
      "    (attention): Attention(\n",
      "      (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    )\n",
      "    (feed_forward): FeedForward(\n",
      "      (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "      (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "      (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    )\n",
      "    (attention_norm): RMSNorm()\n",
      "    (ffn_norm): RMSNorm()\n",
      "  )\n",
      ")\n",
      "layers.0 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.0.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.0.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.0.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.0.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.0.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.0.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.0.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.0.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.0.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.0.attention_norm RMSNorm()\n",
      "layers.0.ffn_norm RMSNorm()\n",
      "layers.1 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.1.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.1.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.1.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.1.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.1.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.1.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.1.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.1.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.1.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.1.attention_norm RMSNorm()\n",
      "layers.1.ffn_norm RMSNorm()\n",
      "layers.2 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.2.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.2.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.2.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.2.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.2.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.2.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.2.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.2.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.2.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.2.attention_norm RMSNorm()\n",
      "layers.2.ffn_norm RMSNorm()\n",
      "layers.3 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.3.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.3.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.3.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.3.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.3.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.3.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.3.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.3.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.3.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.3.attention_norm RMSNorm()\n",
      "layers.3.ffn_norm RMSNorm()\n",
      "layers.4 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.4.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.4.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.4.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.4.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.4.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.4.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.4.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.4.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.4.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.4.attention_norm RMSNorm()\n",
      "layers.4.ffn_norm RMSNorm()\n",
      "layers.5 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.5.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.5.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.5.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.5.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.5.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.5.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.5.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.5.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.5.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.5.attention_norm RMSNorm()\n",
      "layers.5.ffn_norm RMSNorm()\n",
      "layers.6 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.6.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.6.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.6.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.6.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.6.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.6.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.6.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.6.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.6.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.6.attention_norm RMSNorm()\n",
      "layers.6.ffn_norm RMSNorm()\n",
      "layers.7 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.7.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.7.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.7.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.7.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.7.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.7.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.7.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.7.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.7.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.7.attention_norm RMSNorm()\n",
      "layers.7.ffn_norm RMSNorm()\n",
      "layers.8 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.8.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.8.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.8.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.8.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.8.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.8.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.8.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.8.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.8.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.8.attention_norm RMSNorm()\n",
      "layers.8.ffn_norm RMSNorm()\n",
      "layers.9 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.9.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.9.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.9.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.9.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.9.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.9.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.9.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.9.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.9.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.9.attention_norm RMSNorm()\n",
      "layers.9.ffn_norm RMSNorm()\n",
      "layers.10 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.10.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.10.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.10.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.10.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.10.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.10.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.10.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.10.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.10.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.10.attention_norm RMSNorm()\n",
      "layers.10.ffn_norm RMSNorm()\n",
      "layers.11 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.11.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.11.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.11.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.11.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.11.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.11.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.11.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.11.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.11.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.11.attention_norm RMSNorm()\n",
      "layers.11.ffn_norm RMSNorm()\n",
      "layers.12 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.12.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.12.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.12.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.12.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.12.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.12.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.12.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.12.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.12.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.12.attention_norm RMSNorm()\n",
      "layers.12.ffn_norm RMSNorm()\n",
      "layers.13 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.13.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.13.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.13.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.13.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.13.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.13.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.13.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.13.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.13.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.13.attention_norm RMSNorm()\n",
      "layers.13.ffn_norm RMSNorm()\n",
      "layers.14 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.14.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.14.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.14.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.14.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.14.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.14.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.14.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.14.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.14.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.14.attention_norm RMSNorm()\n",
      "layers.14.ffn_norm RMSNorm()\n",
      "layers.15 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.15.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.15.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.15.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.15.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.15.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.15.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.15.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.15.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.15.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.15.attention_norm RMSNorm()\n",
      "layers.15.ffn_norm RMSNorm()\n",
      "layers.16 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.16.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.16.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.16.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.16.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.16.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.16.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.16.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.16.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.16.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.16.attention_norm RMSNorm()\n",
      "layers.16.ffn_norm RMSNorm()\n",
      "layers.17 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.17.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.17.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.17.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.17.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.17.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.17.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.17.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.17.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.17.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.17.attention_norm RMSNorm()\n",
      "layers.17.ffn_norm RMSNorm()\n",
      "layers.18 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.18.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.18.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.18.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.18.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.18.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.18.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.18.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.18.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.18.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.18.attention_norm RMSNorm()\n",
      "layers.18.ffn_norm RMSNorm()\n",
      "layers.19 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.19.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.19.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.19.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.19.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.19.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.19.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.19.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.19.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.19.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.19.attention_norm RMSNorm()\n",
      "layers.19.ffn_norm RMSNorm()\n",
      "layers.20 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.20.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.20.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.20.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.20.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.20.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.20.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.20.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.20.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.20.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.20.attention_norm RMSNorm()\n",
      "layers.20.ffn_norm RMSNorm()\n",
      "layers.21 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.21.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.21.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.21.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.21.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.21.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.21.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.21.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.21.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.21.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.21.attention_norm RMSNorm()\n",
      "layers.21.ffn_norm RMSNorm()\n",
      "layers.22 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.22.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.22.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.22.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.22.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.22.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.22.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.22.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.22.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.22.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.22.attention_norm RMSNorm()\n",
      "layers.22.ffn_norm RMSNorm()\n",
      "layers.23 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.23.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.23.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.23.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.23.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.23.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.23.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.23.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.23.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.23.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.23.attention_norm RMSNorm()\n",
      "layers.23.ffn_norm RMSNorm()\n",
      "layers.24 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.24.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.24.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.24.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.24.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.24.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.24.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.24.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.24.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.24.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.24.attention_norm RMSNorm()\n",
      "layers.24.ffn_norm RMSNorm()\n",
      "layers.25 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.25.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.25.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.25.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.25.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.25.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.25.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.25.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.25.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.25.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.25.attention_norm RMSNorm()\n",
      "layers.25.ffn_norm RMSNorm()\n",
      "layers.26 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.26.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.26.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.26.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.26.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.26.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.26.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.26.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.26.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.26.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.26.attention_norm RMSNorm()\n",
      "layers.26.ffn_norm RMSNorm()\n",
      "layers.27 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.27.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.27.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.27.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.27.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.27.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.27.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.27.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.27.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.27.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.27.attention_norm RMSNorm()\n",
      "layers.27.ffn_norm RMSNorm()\n",
      "layers.28 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.28.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.28.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.28.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.28.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.28.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.28.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.28.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.28.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.28.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.28.attention_norm RMSNorm()\n",
      "layers.28.ffn_norm RMSNorm()\n",
      "layers.29 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.29.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.29.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.29.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.29.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.29.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.29.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.29.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.29.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.29.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.29.attention_norm RMSNorm()\n",
      "layers.29.ffn_norm RMSNorm()\n",
      "layers.30 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.30.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.30.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.30.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.30.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.30.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.30.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.30.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.30.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.30.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.30.attention_norm RMSNorm()\n",
      "layers.30.ffn_norm RMSNorm()\n",
      "layers.31 TransformerBlock(\n",
      "  (attention): Attention(\n",
      "    (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (feed_forward): FeedForward(\n",
      "    (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  )\n",
      "  (attention_norm): RMSNorm()\n",
      "  (ffn_norm): RMSNorm()\n",
      ")\n",
      "layers.31.attention Attention(\n",
      "  (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "layers.31.attention.wq Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.31.attention.wk Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.31.attention.wv Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.31.attention.wo Linear(in_features=4096, out_features=4096, bias=False)\n",
      "layers.31.feed_forward FeedForward(\n",
      "  (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      ")\n",
      "layers.31.feed_forward.w1 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.31.feed_forward.w2 Linear(in_features=11008, out_features=4096, bias=False)\n",
      "layers.31.feed_forward.w3 Linear(in_features=4096, out_features=11008, bias=False)\n",
      "layers.31.attention_norm RMSNorm()\n",
      "layers.31.ffn_norm RMSNorm()\n",
      "norm RMSNorm()\n",
      "output Linear(in_features=4096, out_features=30522, bias=False)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 34\u001B[0m\n\u001B[1;32m     31\u001B[0m     output \u001B[38;5;241m=\u001B[39m model(dummy_input)\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOutput shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 34\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[3], line 31\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     28\u001B[0m dummy_input \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m, model_args\u001B[38;5;241m.\u001B[39mvocab_size, (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m128\u001B[39m))\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Run the forward pass and print layer details\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdummy_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOutput shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/llamaNew/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/llamaNew/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/llama/llama/dummyModel.py:221\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, tokens, start_pos)\u001B[0m\n\u001B[1;32m    218\u001B[0m mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m--> 221\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_pos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfreqs_cis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    222\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLayer \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;241m.\u001B[39mlayer_id\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, output shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mh\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    224\u001B[0m h \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(h)\n",
      "File \u001B[0;32m~/miniconda3/envs/llamaNew/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/llamaNew/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/llama/llama/dummyModel.py:190\u001B[0m, in \u001B[0;36mTransformerBlock.forward\u001B[0;34m(self, x, start_pos, freqs_cis, mask)\u001B[0m\n\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    185\u001B[0m     x: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    188\u001B[0m     mask: Optional[torch\u001B[38;5;241m.\u001B[39mTensor],\n\u001B[1;32m    189\u001B[0m ):\n\u001B[0;32m--> 190\u001B[0m     h \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention_norm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_pos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfreqs_cis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\n\u001B[1;32m    192\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    193\u001B[0m     out \u001B[38;5;241m=\u001B[39m h \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeed_forward(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mffn_norm(h))\n\u001B[1;32m    194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/miniconda3/envs/llamaNew/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/llamaNew/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/llama/llama/dummyModel.py:118\u001B[0m, in \u001B[0;36mAttention.forward\u001B[0;34m(self, x, start_pos, freqs_cis, mask)\u001B[0m\n\u001B[1;32m    115\u001B[0m xk \u001B[38;5;241m=\u001B[39m xk\u001B[38;5;241m.\u001B[39mview(bsz, seqlen, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_local_kv_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\n\u001B[1;32m    116\u001B[0m xv \u001B[38;5;241m=\u001B[39m xv\u001B[38;5;241m.\u001B[39mview(bsz, seqlen, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_local_kv_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\n\u001B[0;32m--> 118\u001B[0m xq, xk \u001B[38;5;241m=\u001B[39m \u001B[43mapply_rotary_emb\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfreqs_cis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfreqs_cis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_k \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_k\u001B[38;5;241m.\u001B[39mto(xq)\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_v\u001B[38;5;241m.\u001B[39mto(xq)\n",
      "File \u001B[0;32m~/Desktop/llama/llama/dummyModel.py:58\u001B[0m, in \u001B[0;36mapply_rotary_emb\u001B[0;34m(xq, xk, freqs_cis)\u001B[0m\n\u001B[1;32m     56\u001B[0m xq_ \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mview_as_complex(xq\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m*\u001B[39mxq\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m))\n\u001B[1;32m     57\u001B[0m xk_ \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mview_as_complex(xk\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m*\u001B[39mxk\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m))\n\u001B[0;32m---> 58\u001B[0m freqs_cis \u001B[38;5;241m=\u001B[39m \u001B[43mreshape_for_broadcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfreqs_cis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxq_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m xq_out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mview_as_real(xq_ \u001B[38;5;241m*\u001B[39m freqs_cis)\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m     60\u001B[0m xk_out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mview_as_real(xk_ \u001B[38;5;241m*\u001B[39m freqs_cis)\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m3\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/llama/llama/dummyModel.py:47\u001B[0m, in \u001B[0;36mreshape_for_broadcast\u001B[0;34m(freqs_cis, x)\u001B[0m\n\u001B[1;32m     45\u001B[0m ndim \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mndim\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;241m0\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m<\u001B[39m ndim\n\u001B[0;32m---> 47\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m freqs_cis\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m (x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     48\u001B[0m shape \u001B[38;5;241m=\u001B[39m [d \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m i \u001B[38;5;241m==\u001B[39m ndim \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, d \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape)]\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m freqs_cis\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m*\u001B[39mshape)\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T00:46:06.739295Z",
     "start_time": "2024-06-05T00:46:06.731609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Get the default dtype globally\n",
    "default_dtype = torch.get_default_dtype()\n",
    "print(default_dtype)  # This will print the default dtype\n",
    "\n",
    "# Example output: torch.float32\n"
   ],
   "id": "8cfb7a16a539d813",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T00:23:58.726885Z",
     "start_time": "2024-06-05T00:23:58.715575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Set the default dtype globally\n",
    "torch.set_default_dtype(torch.float16)  # Example: Set it to double precision float\n",
    "\n",
    "# Now, when you create tensors without specifying dtype, they will have the default dtype\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(x.dtype)  # Output: torch.float64\n",
    "\n",
    "# Note: This will only affect tensors created afterwards. Existing tensors will keep their original dtype.\n"
   ],
   "id": "8498cd4ef95c90ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T00:46:18.418486Z",
     "start_time": "2024-06-05T00:46:18.415884Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"hi\")",
   "id": "dc81509d7f6f56bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "493baf3dc1ec0b31"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
